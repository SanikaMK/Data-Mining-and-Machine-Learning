---
title: "Signature Project"
output:
  html_document:
    df_print: paged
---

# Abstract 

The data presented in this document is taken from the USMLE® Step 2 Clinical Skills test [https://www.kaggle.com/competitions/nbme-score-clinical-patient-notes/data?select=patient_notes.csv], which is a licensing exam for medical professionals. The exam evaluates a trainee's ability to recognize important clinical information while interacting with standardized patients. In this test, each participant interacts with a standardized patient, who acts out a medical scenario. The participant then writes down relevant information about the encounter in a patient note, which is then evaluated by a physician. The physician looks for specific key concepts or features related to the case and grades the patient note accordingly. This project involves the analysis of textual data related to medical cases to determine the likelihood of a particular RAF score (a measure of the severity of injuries sustained in an accident). The data was preprocessed to remove noise and irrelevant information and was then used to build a topic model using the LDA algorithm. The resulting topic model was then used to identify the most important terms associated with each topic and to determine the likelihood of a particular document being associated with a particular topic. The analysis was performed using beta and gamma values as RAF scores, and no traditional statistical measures like MAD or MSE were used as the focus was on text analysis rather than numerical data. Overall, this project provides insights into the use of NLP techniques for analyzing medical data and offers a potential tool for predicting the severity of injuries in accident cases.

The **aim** of this project is to create an automated method for identifying the relevant features within the patient note, with a particular emphasis on the patient's medical history section, where the information gathered during the interview with the standardized patient is documented.

The project below will be targeted to follow all the necessary **CRISP- DM** parameter for its successful completion. 

# Definition of the Problem

The problem being addressed in this project is the difficulty of accurately and efficiently identifying key clinical features in patient notes. Currently, the evaluation of these notes is done manually by trained physicians, who look for specific concepts and features related to the case. This process is time-consuming, subjective, and potentially prone to errors. The goal of this project is to develop an automated system that can accurately and efficiently identify the relevant features within the patient notes, with a particular focus on the patient history section. This would help improve the evaluation process of the exam and ensure that medical professionals are properly evaluated on their ability to recognize and document important clinical information.

# Understanding the Data


```{r}
features <- read.csv("/Users/sanikakalvikatte/Downloads/features.csv")
patient_notes <- read.csv("/Users/sanikakalvikatte/Downloads/patient_notes.csv")
train <- read.csv("/Users/sanikakalvikatte/Downloads/train.csv")
```

The above step is loading the data, through read.csv() function in R. This data was taken from Kaggle and belongs to USMLE® Step 2 Clinical Skills examination, which they claim to be dummy patient data, to preserve the HIPAA rights. 

```{r}
features
patient_notes
train
```
Looking at the three main data frames here, which in this scenario are features, patient notes and train data frame. The dataset contains around 40,000 Patient Note history portions in patient_notes.csv file. Only a subset of these notes have features annotated, hence unsupervised learning techniques can be applied to the unannotated notes. The test set patient notes are not included in the public version of this file. The patient notes are identified by unique pn_num, while case_num is used to identify the clinical case a patient note represents. The pn_history column in patient_notes.csv contains the text of the encounter as recorded by the test taker.

The features.csv file contains the rubric of features or key concepts for each clinical case. Each feature is identified by a unique feature_num, and case_num identifies the clinical case. The feature_text column describes each feature.

The train.csv file contains feature annotations for 1000 of the patient notes, 100 for each of the ten cases. Each patient note and feature pair is identified by a unique id. The pn_num, feature_num, and case_num columns identify the patient note, feature, and case respectively. The annotation column contains the text(s) within a patient note indicating a feature. A feature may be indicated multiple times within a single note. The location column indicates the character spans of each annotation within the note, which may require multiple spans delimited by a semicolon (;).

# Exploratory Data Analysis and Assesment

#### Summary

```{r}
summary(features)
summary(patient_notes)
```
From the given summary, we can gather the following information:

The dataset has a total of 143 features with feature_num ranging from 0 to 916.
The dataset has a total of 10 cases with case_num ranging from 0 to 9.
The most frequent feature_text and pn_history are not known from the given summary.
The dataset has a total of 42,146 patient notes with pn_num ranging from 0 to 95,334.
We can also calculate additional summary statistics for the numerical columns:

For feature_num column:

The median is 502.0, which is closer to the 75th percentile than the 25th percentile, indicating a right-skewed distribution.
The mean is 466.4, which is slightly lower than the median, further indicating a right-skewed distribution.
The range is from 0.0 to 916.0, with an interquartile range (IQR) of 498.0 (from 209.5 to 707.5).
For case_num column:

The median is 5.000, which is closer to the mean than the minimum or maximum value.
The mean is 4.594, indicating a relatively evenly distributed cases.
The range is from 0.000 to 9.000, with an IQR of 5.000 (from 2.000 to 7.000).
For pn_num column:

The median is 50912, which is higher than the mean, indicating a slightly left-skewed distribution.
The mean is 52440, indicating a relatively evenly distributed pn_num.
The range is from 0 to 95,334, with an IQR of 37,020 (from 35,618 to 73,038).


#### Data Type

```{r}
str(features)
str(patient_notes)
```

The first data frame contains 143 observations of 3 variables: feature_num (integer), case_num (integer), and feature_text (character).

The second data frame contains 42,146 observations of 3 variables: pn_num (integer), case_num (integer), and pn_history (character).

#### Missing Data

```{r}
any(is.na(features))
any(is.na(patient_notes))
any(is.na(train))
```

In data analysis and modeling, missing values can be a common occurrence in data sets. They can arise due to a variety of reasons such as human error, data collection errors, or simply due to the nature of the data itself. In some cases, missing values can be easily identified using functions like is.na() which returns a logical vector indicating whether each value in a given vector or data frame is missing or not.

However, in some cases, missing values can be hidden or not immediately apparent. This is what the above statement is referring to. At first glance, it may appear that there are no missing values in the data set. However, upon further examination of the train data frame, it was discovered that the annotation field has some missing values that were not flagged as NA by the is.na() function.

This situation is not uncommon and can be problematic when it comes to data analysis and modeling. The presence of missing values can lead to biased results and inaccurate conclusions. Therefore, it is important to identify and address missing values appropriately.

In this case, the statement suggests that data imputation might be required. Data imputation refers to the process of replacing missing values with plausible estimates based on the available data. There are several methods for data imputation such as mean imputation, regression imputation, and multiple imputation. The choice of method depends on the nature of the data and the research question at hand.

In summary, the above statement highlights the importance of thorough data exploration and the potential presence of hidden missing values in data sets. It also suggests that appropriate measures such as data imputation may be necessary to address missing values and obtain reliable results.


#### Duplicate Data

```{r}
anyDuplicated(features)
anyDuplicated(patient_notes)
```

Checking for duplicates, this step here is performed to do better analysis work, to understand the duplicay in the important data frames. 

#### Outliers

```{r}
boxplot(features$feature_num)
boxplot(features$case_num)
boxplot(patient_notes$pn_num)
boxplot(patient_notes$case_num)
```

In the above code, four boxplots are being generated, each for a different variable in the data sets: feature_num and case_num in the features data frame, and pn_num and case_num in the patient_notes data frame. By using the boxplot() function on each variable, the data distribution and any potential outliers can be visualized and assessed. It suggests that after generating the boxplots for each variable, no points were observed outside the whiskers of the boxplot for any of the variables. The whiskers are lines extending from the box indicating the range of the data, and any points outside the whiskers are considered outliers. In summary, the above code generates boxplots for different variables in two data sets and suggests that no outliers were observed in any of the variables. This is a useful observation for data analysis and can help to identify any potential issues with the data set.

#### Merge Datasets

```{r}
library(dplyr)
merge_df <- left_join(features, train, by= c("case_num", "feature_num"))
merge_df
final_df <- left_join(merge_df, patient_notes, by=c("pn_num","case_num"))

final_df
```

```{r}
# Convert the feature_text variable to a factor
final_df$feature_text <- factor(final_df$feature_text)

# Create a contingency table
cont_table <- table(final_df$feature_text, final_df$id)

# Perform the chi-squared test
chisq <- chisq.test(cont_table)
chisq
```
In the given result, the test statistic is X-squared = 1859000 and the degrees of freedom (df) is 1858870. The p-value is 0.473, which means there is not enough evidence to reject the null hypothesis that the two categorical variables are independent. This implies that there is no significant association between the two variables based on the data provided.

In the above code is using the dplyr library in R to merge three data frames: features, train, and patient_notes, based on specific columns in each data frame.

First, the code uses the left_join() function from the dplyr package to merge the features and train data frames. The left_join() function is a type of join that keeps all the rows from the left data frame (in this case, the features data frame) and merges any matching rows from the right data frame (in this case, the train data frame) based on the specified column names. The "by" parameter in the left_join() function specifies the columns to be used for the merge, which in this case are "case_num" and "feature_num".

The resulting merged data frame is stored in the "merge_df" object, and the code prints the merged data frame to the console using the print() function.

Next, the code uses another left_join() function to merge the "patient_notes" data frame with the "merge_df" data frame. The "by" parameter in this left_join() function specifies the columns to be used for the merge as "pn_num" and "case_num". The resulting merged data frame is stored in the "final_df" object.

Overall, this code is useful for combining information from multiple data frames into one, by joining the data frames based on common columns. This can be helpful in situations where different data sets have complementary information, and combining them can provide a more comprehensive view of the data. The dplyr library provides an efficient and flexible way to perform these kinds of data manipulations, making it a popular choice among data analysts and data scientists in R.

# Data Transformation

```{r}
final_df$feature_num <- scale(final_df$feature_num, center = TRUE, scale = TRUE)
final_df$case_num <- scale(final_df$case_num, center = TRUE, scale = TRUE)
final_df$pn_num <- scale(final_df$pn_num, center = TRUE, scale = TRUE)
final_df$case_num <- scale(final_df$case_num, center = TRUE, scale = TRUE)
final_df
any(is.na(final_df))
```

The above code performs scaling on several columns in the "final_df" data frame using the scale() function in R.

The scale() function standardizes the data by centering it on the mean and scaling it to unit variance. In other words, it subtracts the mean value of the variable from each observation and then divides by the standard deviation of the variable. This transformation makes the variables comparable and easier to interpret since they are now in the same units of measurement.

Please note, we will be de-normalizing once the model is ready. This step is done to get the data in a normalized way and to remove any model biases that can potentially occur. The "center = TRUE" argument specifies that the data should be centered (i.e., subtract the mean), and the "scale = TRUE" argument specifies that the data should be scaled (i.e., divided by the standard deviation).

# Splitting the Data

```{r}
library(rsample)
set.seed(12345)

# final_df$feature_text
# a <- unique(final_df$feature_text)
# a <- a[1:10]
train_indices <- initial_split(final_df, prop = 0.7)
train_data <- training(train_indices)
test_data <- testing(train_indices)

train_data_nn <-train_data
test_data_nn <- test_data

train_data <- train_data[8]
test_data <- test_data[8]
```

The percentage of data that should go into training and testing datasets can vary depending on the specific problem and dataset. However, there are some general guidelines that can help you decide on an appropriate split:

The most common split is to use 70-80% of the data for training and 20-30% for testing. This is a good starting point for most problems.

If you have a very large dataset, you can use a smaller percentage for testing, such as 10-20%. This can still provide enough data for testing while allowing for a larger training set.

Since in this scenario, we have a data set of about 40k observations, which is a standard data set as compared with the assignments, I have decide to split the data in a standard way of 70% train and 30% test data. 

# Naive Bayes Modelling.

The initial step looking at the data set, I thought was it needs text classification. The Naive Bayes model is useful, as prior we have done a text classification example for sms sampling and segregation. The first model I am going to try is through Naive Bayes, although one drawback I see is it has multilabels which will make the classification matrics be very heavy and hard to understand visually.

```{r}
library(tm)
library(dplyr)
str(final_df$pn_history)
final_df$pn_history <- factor(final_df$pn_history)
str(final_df$pn_history)
corpus <- VCorpus(VectorSource(final_df$pn_history))
inspect(corpus[1:2])
as.character(corpus[[1]])
lapply(corpus[1:2], as.character)
corpus_clean <- tm_map(corpus, content_transformer(tolower))
as.character(corpus[[1]])
as.character(corpus_clean[[1]])
corpus_clean <- tm_map(corpus_clean, removeNumbers)
corpus_clean <- tm_map(corpus_clean, removeWords, stopwords())
corpus_clean <- tm_map(corpus_clean, removePunctuation)
corpus_clean <- tm_map(corpus_clean, stemDocument)
corpus_clean <- tm_map(corpus_clean, stripWhitespace)
dtm <- DocumentTermMatrix(corpus_clean)
dtm2 <- DocumentTermMatrix(corpus, control = list(
  tolower = TRUE,
  removeNumbers = TRUE,
  stopwords = TRUE, 
  removePunctuation = TRUE,
  stemming = TRUE
))
dtm2
dtm
dtm_train <- dtm[1:10010,]
dtm_test <- dtm[10011:14300,]
dtm_train


train_labels <- final_df[1:10010,]$feature_text
unique(train_labels)
test_labels <- final_df[10011:14300,]$feature_text
```
This R code performs text preprocessing and prepares the data for classification by generating a document-term matrix. The code performs text preprocessing and prepares the data for classification by generating a document-term matrix.

The tm package is used for text mining, and the dplyr package is used for data manipulation. The VCorpus function is used to create a corpus from the "pn_history" column of "final_df", which is then preprocessed using a series of tm_map functions. These functions include converting the text to lowercase, removing numbers, removing stopwords and punctuation, stemming, and stripping whitespace.

Two document-term matrices are generated: dtm and dtm2. dtm is generated from the preprocessed corpus, while dtm2 is generated from the original corpus using a control list of options. The purpose of generating two matrices is not entirely clear from the code.

The DocumentTermMatrix function is used to generate the matrices, and the resulting matrices are then split into training and testing sets using indexing. The labels for the training and testing sets are extracted from the "final_df" dataframe and stored in the train_labels and test_labels variables, respectively.

```{r}
#install.packages("wordcloud")
library(wordcloud)
suppressWarnings({
  wordcloud(corpus_clean,
          max.words = 1000, random.order = FALSE,
          rot.per = 0.5, colors = brewer.pal(8, "Dark2"), min.freq = 5, scale = c(4, 0.5))
})
```

The code generates a wordcloud using the wordcloud package. The corpus_clean variable is the input for the wordcloud function, and it represents a preprocessed corpus.

The max.words argument is set to 1000, which specifies the maximum number of words to display in the wordcloud. The random.order argument is set to FALSE, which means the words will be displayed in decreasing order of frequency. The rot.per argument specifies the proportion of words to rotate. The colors argument sets the color palette for the wordcloud, and the min.freq argument specifies the minimum frequency of a word for it to be included in the wordcloud. The scale argument adjusts the size of the words in the wordcloud.

The suppressWarnings() function is used to suppress any warnings that might occur during the execution of the wordcloud function. As we are tring to fit a lot of words taken from the paragrahs worth of sentences, into a frame of top 1000 frequent words, this cause R to give warning to inform about the eliminated words, which are very high, and an individual needs to scroll through the warnings, to reach to the wordcloud analysis.

Something about the wordcloud is that it is generated using colours to segregate the information, that way the comparative visualization is quick and easy. The most frequent word in the data frame is "pain", it is not surprising that it would also be the most frequent word in the wordcloud generated by the code you provided. The min.freq argument is set to 5, which means that words with a frequency less than 5 will not be included in the wordcloud. However, if "pain" appears more than 5 times in the corpus, it will likely be included in the wordcloud. 

While the wordcloud provides a quick visualization of the most frequent words in the corpus, it is important to note that the wordcloud is limited by its design and may not be the best tool for in-depth analysis of text data. It is generally recommended to use other text mining techniques, such as topic modeling or sentiment analysis, for more comprehensive analysis of text data. 

So for this reason, I am going to perform, modelling through two different ways, they are text mining through Naive Bayes and topic modelling through LDA. 

```{r}
library(tm)
freq_words <- findFreqTerms(dtm_train,5)
str(freq_words)
dtm_freq_train <- dtm_train[, freq_words]
dtm_freq_test <- dtm_test[, freq_words]
convert_counts <- function(x){
  x <- ifelse(x > 0, "Yes", "No")
}

library(e1071)
classifier <- naiveBayes(train_data, train_labels)
predict <- predict(classifier, test_data)
```

The code first loads the tm package, which provides a framework for managing and manipulating text data.

The findFreqTerms function is then used to identify the most frequent words in the training set dtm_train, with a minimum frequency of 5. The str function is used to print the structure of the freq_words object, which is a character vector containing the most frequent words.

The code then creates two new document-term matrices (dtm_freq_train and dtm_freq_test) based on the most frequent words in the training set identified earlier. These matrices will be used for training and testing a Naive Bayes classifier.

The convert_counts function is then defined, which is used to convert the counts in the document-term matrices to binary variables ("Yes" or "No").

Next, the e1071 package is loaded, which contains the naiveBayes function for fitting a Naive Bayes classifier. The train_data and train_labels variables are assumed to be defined elsewhere in the code and are used to fit the classifier.

Finally, the predict function is used to predict the class labels of the test data (test_data). The predicted labels are stored in a variable called predict. It's worth noting that using predict as a variable name is not recommended since it can also be a function name.


```{r}
# library(gmodels)
# CrossTable(predict, test_labels,
#     prop.chisq = FALSE, prop.c = FALSE, prop.r = FALSE,
#     dnn = c('predicted', 'actual'))
```

```{r, results = 'hide'}
#install.packages("caret")
library(caret)
a <- as.factor(predict[1:40])
pred <- as.factor(test_labels[1:40])
x <- confusionMatrix(pred, a)
#x 
```

The output of the confusion matrix suggests that the classifier has very poor performance on the given dataset. The sensitivity and positive predictive value are both reported as NaN for one of the classes, indicating that the classifier did not correctly identify any instances of that class. The specificity and negative predictive value are both high, indicating that the classifier did well at correctly identifying instances that were not part of that class. The prevalence of the class is low, at only 0.05, indicating that it is a relatively rare class. The overall balanced accuracy is only 0.5, indicating that the classifier is no better than random guessing. Overall, these results suggest that the classifier is not performing well on the given dataset and may require further tuning or evaluation.

### Model Drawback and Learnings:

Looking at the data frame having multilabels, the classifers is able to provide multilevel output, however it is hard for the visual eye to understand and provide analysis. In the context of text classification with multilabel data, the goal is often to assign one or more labels to a given text document. This means that a document can belong to multiple classes or categories at the same time. For example, a news article might be about both politics and sports, or a movie review might be positive and also mention the acting and the direction. One approach to classifying multilabel text data is to use a classifier that can assign multiple labels to a document. In the code provided, a Naive Bayes classifier is used to predict the labels of the test data. This classifier can handle multilabel data and assign multiple labels to a document if appropriate. However, since there are 143 unique parameters for the classifer it still stand as a not so suitable model for this scenario. Thus, at this point, topic modelling seems a better option for the given situation. The next model after Naive Bayes is going to be LDA for topic modelling. 

# Hyperparameter Tuning for NB: 

```{r}
classifier2 <- naiveBayes(train_data, train_labels, laplace = 1)
pred2 <- predict(classifier2, test_data)
```

Laplace smoothing is a hyperparameter in Naive Bayes that is used to avoid zero probability estimates for features that do not appear in the training data. A common value for Laplace smoothing is 1, which means that each feature count is increased by 1 before computing probabilities. This helps to avoid zero probabilities and improves the generalization performance of the model. However, the optimal value of Laplace smoothing can vary depending on the dataset and the specific problem being solved. Therefore, it is important to tune this hyperparameter to achieve the best possible performance of the Naive Bayes model.

```{r}
# library(gmodels)
# CrossTable(pred2, test_labels,
#     prop.chisq = FALSE, prop.c = FALSE, prop.r = FALSE,
#     dnn = c('predicted', 'actual'))
```

#  Topic Modelling through Latent Dirichlet Allocation (LDA).
 
```{r}
library(tidyverse)
library(plotly)
library(ggplot2)
library(stringr)
#install.packages("tensorflow")
library(reticulate)
library(tensorflow)

options(width = 9999)

```

Starting new with a new type of modelling. LDA stands for Latent Dirichlet Allocation. It is a statistical model used in natural language processing for topic modeling, which is the process of identifying topics within a collection of documents. LDA is a generative probabilistic model that assumes each document is a mixture of a small number of topics, and each word in the document is attributable to one of the document's topics. LDA has become a popular method for analyzing large text corpora because it provides a way to automatically discover hidden topics in a collection of documents.

### Data Cleaning And Preprocessing

```{r}
length(unique(train$case_num))
length(unique(train$pn_num))
length(unique(train$feature_num))
```

The code returns the length of unique values for three variables: "case_num", "pn_num", and "feature_num" in the "train" dataset. The value returned for "case_num" is 10, indicating that there are only 10 unique cases in the dataset. The value returned for "pn_num" is 1000, indicating that there are 1000 unique patent numbers in the dataset. The value returned for "feature_num" is 143, indicating that there are 143 unique features in the dataset. These results suggest that the dataset is relatively small and may not represent a large and diverse sample of patent features. Therefore, the results and conclusions drawn from any analysis of this dataset may not be generalizable to the population of patent features.

```{r}
cat("Train Data Shape (", dim(train), ")\n")
```

The code is displaying the shape of the "train" data using the "dim" function, which returns the number of rows and columns in a data frame. In this case, it shows that the "train" data has 14300 rows and 6 columns. Analytically, knowing the size of the data can be useful for various tasks, such as determining how much memory the data takes up or how much data cleaning and preprocessing may be necessary. It also allows for better planning and understanding of the subsequent steps in the data analysis process.

```{r}
num_empty_annotation <- train[train$location == "[]",]
cat("Number of rows where there is no annotation ", nrow(num_empty_annotation), "\n")
```

The code is identifying the number of rows where the location column has an empty annotation, denoted by "[]". The variable num_empty_annotation is created as a subset of the train data frame where the location is empty. The function nrow() is then used to count the number of rows in num_empty_annotation. The output of the code is a message that prints the number of rows with empty annotations, which is 4399. This information could be useful for identifying potential issues in the data, such as missing information or incomplete data entries. It may also be relevant for data cleaning or preprocessing steps.

```{r}
train$annotation <- sapply(train$annotation, eval)
train$location <- sapply(train$location, eval)
```


```{r}
head(train)
```

```{r}
train$number_annotations <- sapply(train$annotation, function(x) nchar(x))
head(train)
```

Overall comment for the above pre-processing of the data, These lines of code are used to preprocess the train data frame. sapply() function is used to apply the eval() function to each element of the annotation and location columns of the train data frame. The eval() function is used to evaluate a string as a command, which means it is used to execute the R code in the character strings in the annotation and location columns. This is done to convert the character string in these columns to an R object type to make it easier to extract the relevant information.

The third line of code uses sapply() function to apply an anonymous function to the annotation column of the train data frame. This anonymous function calculates the number of characters in each annotation and returns a numeric vector of the number of characters in each annotation. This result is stored in the new number_annotations column of the train data frame.

```{r}
anno_count <- table(train$number_annotations)
anno_count_df <- data.frame(Count = as.integer(anno_count), Number_of_Annotations = as.integer(names(anno_count)))

library(plotly)
fig <- plot_ly(anno_count_df, x = ~Number_of_Annotations, y = ~Count, type = "bar", color = ~Number_of_Annotations)
fig <- fig %>% layout(xaxis = list(title = "Number of Annotations", range = c(0, 10)), yaxis = list(title = "Annotations Count", range = c(0, 8000)))
fig

```

To check the number of annotations to the annotation count in the field, plotly comes as useful tool. It can create a great heat wave, if the data is as needed. Then, the plot_ly() function is used to create a plot object with the data frame anno_count_df as input. The x and y arguments specify the variables to be plotted on the x and y axis, respectively, and the type argument specifies the plot type as a bar chart. The color argument is used to color the bars based on the number of annotations.

```{r}
annotations_list <- train$annotation
annotations <- unlist(annotations_list)
cat("Number of Unique Annotations: ", length(unique(annotations)), "\n")

lower_annotations <- tolower(annotations)
cat("Number of Unique Annotations after converting to lower: ", length(unique(lower_annotations)))
```

```{r}
top_n <- 50
lower_annotations_count <- table(lower_annotations)
most_common_annotations <- head(sort(lower_annotations_count, decreasing = TRUE), top_n)
most_common_annotations
```

```{r}
head(train)
```

```{r}
train
train[1397,]
```

```{r}
patient_notes[patient_notes$case_num == 0 & patient_notes$pn_num == 669,]
```
In the above example: The right annotation should be "father heart attack" instead of "ather heart attack"

```{r}
train$annotation[622]
train$annotation[656]
train$annotation[1263]
train$annotation[1266]
train$annotation[1397] 

train$annotation[1592] 

train$annotation[1616]

train$annotation[1665] 

train$annotation[1715] 
train$annotation[1930] 

train$annotation[2135] 

train$annotation[2192] 

train$annotation[2554] 

train$annotation[3125] 

train$annotation[3859] 

train$annotation[4374] 

train$annotation[4764] 
train$annotation[4783] 

train$annotation[4909] 

train$annotation[6017] 

train$annotation[6193] 

train$annotation[6381] 

train$annotation[6563] 

train$annotation[6863] 

train$annotation[7023] 

train$annotation[7423] 

train$annotation[8877] 

train$annotation[9028] 

train$annotation[9939] 

train$annotation[9974] 

train$annotation[10514] 

train$annotation[11552] 

train$annotation[11678] 

train$annotation[12125] 

train$annotation[12280] 

train$annotation[12290]
train$annotation[13239] 

train$annotation[13298]

train$annotation[13300] 

train$annotation[13846] 

train$annotation[14084] 

```

```{r}
train$annotation <- gsub("ather heart", "father heart", train$annotation)
train$annotation <- gsub("or the last", "for the last", train$annotation )
train$annotation <- gsub("felt ike he", "felt like he", train$annotation )
train$annotation <- gsub("no old intolerance", "no cold intolerance", train$annotation )
train$annotation <- gsub("mother hyroid problem", "mother thyroid problem", train$annotation )
train$annotation <- gsub("no blooody", "non blooody", train$annotation )
train$annotation <- gsub("no v", "no vignal", train$annotation )
train$annotation <- gsub("hours ag", "hours ago", train$annotation )
train$annotation <- gsub("blood n the", "blood in the", train$annotation )
train$annotation <- gsub("ast sexually", "last sexually", train$annotation )
train$annotation <- gsub("ight lower quadrant pain", "light lower quadrant pain", train$annotation )
train$annotation <- gsub("previously asting 5 days", "previously lasting 5 days", train$annotation )
train$annotation <- gsub("stressor aking care", "stressor taking care", train$annotation )
train$annotation <- gsub("seeing her son knows", "seeing her son nows", train$annotation )
train$annotation <- gsub("peptic ulce", "peptic ulcer", train$annotation )
train$annotation <- gsub("ifficulty falling", "difficulty falling", train$annotation )
train$annotation <- gsub("elps to take", "helps to take", train$annotation )
train$annotation <- gsub("seeing her son knows", "seeing her son nows", train$annotation )
train$annotation <- gsub("seeing her son knows", "seeing her son nows", train$annotation )
train$annotation <- gsub("aw him once", "saw him once", train$annotation )
train$annotation <- gsub("not actually happe", "not actually happen", train$annotation )
train$annotation <- gsub("hea", "head", train$annotation )
train$annotation <- gsub("ffor the ", "for the ", train$annotation )
train$annotation <- gsub("headache globa", "headache global", train$annotation )
train$annotation <- gsub("headd", "head", train$annotation )
train$annotation <- gsub("stressed due to ttttaking care", "stressed due to taking care", train$annotation )
train$annotation <- gsub("llllike", "like", train$annotation )
```

```{r}
train$annotation[622]
train$annotation[656]
train$annotation[1263]
train$annotation[1266]
train$annotation[1397] 

train$annotation[1592] 

train$annotation[1616]

train$annotation[1665] 

train$annotation[1715] 
train$annotation[1930] 

train$annotation[2135] 

train$annotation[2192] 

train$annotation[2554] 

train$annotation[3125] 

train$annotation[3859] 

train$annotation[4374] 

train$annotation[4764] 
train$annotation[4783] 

train$annotation[4909] 

train$annotation[6017] 

train$annotation[6193] 

train$annotation[6381] 

train$annotation[6563] 

train$annotation[6863] 

train$annotation[7023] 

train$annotation[7423] 

train$annotation[8877] 

train$annotation[9028] 

train$annotation[9939] 

train$annotation[9974] 

train$annotation[10514] 

train$annotation[11552] 

train$annotation[11678] 

train$annotation[12125] 

train$annotation[12280] 

train$annotation[12290]
train$annotation[13239] 

train$annotation[13298]

train$annotation[13300] 

train$annotation[13846] 

train$annotation[14084] 

```

```{r}
head(train)
train
```

```{r}
library(dplyr)

train <- merge(train, features, by = c("feature_num", "case_num"), all.x = TRUE)
train <- merge(train, patient_notes, by = c("pn_num", "case_num"), all.x = TRUE)
head(train)
```

```{r}
unique(patient_notes[c("pn_num", "case_num")]) %>% dim()
unique(train[c("pn_num", "case_num")]) %>% dim()
```

After the preprocessing, we can aim for topic modelling,just refreshing the aim here is to get the probability of location which in this scenario is considered as the RAF score. The text preprocessing steps have been completed and now the focus is on topic modelling to obtain the probability of location, which is being considered as the RAF (Risk Assessment Factor) score in this scenario. Topic modelling is a statistical technique that aims to identify the underlying topics or themes that exist within a set of text data. The idea is to extract meaningful and interpretable information from the text data by identifying patterns and relationships between words and phrases. In this case, the aim is to identify the topics related to the location information present in the text data, which will be used as a proxy for the RAF score. Once the topics have been identified, the probability of each topic can be calculated for each document in the dataset, which can then be used as a predictor for the RAF score.

```{r}
library(topicmodels)
library(lda)
LDA_model <- LDA(dtm2, k = 4, control = list(seed = 1234))
library(tidytext)
topics_model <- tidy(LDA_model, matrix = "beta")
```

The code above loads the necessary libraries topicmodels, lda, and tidytext for topic modeling. It then creates an LDA model by applying the LDA() function to the dtm2 document-term matrix with four topics (k = 4). The control parameter is used to set the random seed for reproducibility purposes. Next, the tidy() function from the tidytext library is applied to the LDA model output, which converts the output into a tidy data frame format for easier analysis and visualization. The resulting data frame, topics_model, contains the topic term probabilities for each topic and term pair. This data frame can be used to explore the top terms for each topic, as well as the overall topic structure of the corpus.

```{r}
top_terms <- topics_model %>%
  group_by(topic) %>%
  slice_max(beta, n = 5) %>% 
  ungroup() %>%
  arrange(topic, -beta)

top_terms
```

The code is generating the top 5 terms in each topic. It first groups the terms by topic, then selects the top 5 terms with the highest probability (beta) for each topic using the slice_max function. The resulting table is then sorted in descending order by beta for each topic using the arrange function. The resulting top_terms table displays the top 5 terms in each topic along with their corresponding beta values. This information can be used to interpret the topics generated by the LDA model. 

The data shows the top 5 terms with the highest beta values for each of the 2 topics identified by the LDA model. The term "pain" appears in both topics and has a relatively high beta value in each. In topic 1, the terms "last", "deni", "day", and "none" have the next highest beta values, while in topic 2, the terms "chang", "day", "deni", and "ago" have the next highest beta values. These top terms can give us an idea of the topics that each of the identified topics might be representing, such as pain experiences and time-related factors (days, last, ago).

```{r}
top_terms_gamma <- tidy(LDA_model, matrix = "gamma")
top_terms_gamma
```

The code creates a new data frame named top_terms_gamma by extracting gamma values from the LDA model using the tidy() function from the tidytext package. Gamma values are the probabilities of each document belonging to a particular topic, and the resulting data frame has three columns: document, topic, and gamma. This data frame shows the distribution of topics across documents in the corpus.

The data shows the topic distribution for each document in the LDA model. Each row represents a document with its corresponding topic and the gamma value, which represents the proportion of that topic in the document. In this case, the document column is of character type and represents the text of the document. The topic column is of integer type and represents the topic number assigned to the document. The gamma column is of double type and represents the proportion of that topic in the document. The data shows the topic distribution for the first ten documents, with document 1 having a gamma value of 0.2435526 for topic 1, document 2 having a gamma value of 0.2530157 for topic 1, and so on.



```{r}
library(ggplot2)

top_terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(beta, term, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  scale_y_reordered()
```

Overall, after performing topic modeling, we have obtained the RAF scores for each term in each topic. The beta values represent the probability of a term belonging to a particular topic, while the gamma values represent the probability of a document belonging to a particular topic. By analyzing these scores, we can gain insights into the topics present in the corpus and the documents that are most relevant to each topic.

For example, in topic 1, the terms "pain," "last," and "deny" have high beta scores, indicating that they are strongly associated with this topic. In addition, documents 1-10 have high gamma scores for topic 1, suggesting that they are highly relevant to this topic.

# Pros and Cons of Using LDA

The given data represents the result of topic modeling using Latent Dirichlet Allocation (LDA) with k=2 topics. The first part of the data shows the top terms for each of the two topics along with their beta values, which indicate the probability of a term being associated with a particular topic. For example, in topic 1, the term "pain" has the highest probability of being associated with the topic (beta=0.040), followed by "last" (beta=0.026) and "deni" (beta=0.019). Similarly, in topic 2, "chang" has the highest probability of being associated with the topic (beta=0.021), followed by "pain" (beta=0.021) and "day" (beta=0.018). The second part of the data shows the gamma values, which represent the probability of each document being associated with each topic. For example, in the first document, the gamma value for topic 1 is 0.244, while the gamma value for topic 2 is 0.756, indicating that the first document is more likely to be associated with topic 2 than with topic 1. These gamma values can be used as the RAF score for each document to predict the likelihood of the document belonging to each topic.

The cons here are MAD (Mean Absolute Deviation) and MSE (Mean Squared Error) are statistical metrics commonly used to measure the accuracy of a predictive model by comparing the predicted values with the actual values. However, in the context of NLP (Natural Language Processing), where the data is primarily in the form of text, these metrics may not be applicable or relevant. This is because NLP involves analyzing and processing human language, which is complex and subjective, and cannot be easily quantified in terms of a numerical value that can be used for computing errors.

# LDA with Open NLP

The model model that I tried is commented out because on Mac the openNLP library does not work, I did get a chance to work it out on windows, however, the LDA has a large corpus of data to train on and lowering the corpus doesn't make sense and it will also not produce valuable results. The LDA model needs higher GPU space for this evalation of this model. 

```{r}
# suppressMessages(library(readr))
# suppressMessages(library(plyr))
# suppressMessages(library(dplyr))
# suppressMessages(library(magrittr))
# suppressMessages(library(data.table))
# suppressMessages(library(datasets))
# suppressMessages(library(tidyverse))
# suppressMessages(library(tidytext))
# suppressMessages(library(stringr))
# suppressMessages(library(syuzhet))
# suppressMessages(library(tm))
# suppressMessages(library(stm))
# suppressMessages(library(spacyr))
# suppressMessages(library(NLP))
# suppressMessages(library(openNLP))
# suppressMessages(library(topicmodels))
# suppressMessages(library(randomForest))
# suppressMessages(library(modelplotr))
# suppressMessages(library(ggplot2))
# suppressMessages(library(knitr))
```

```{r}
# print(paste("First row of train dataset:"))
# train[1,]
# 
# print(paste("First row of features dataset:"))
# features[1,]
# 
# print(paste("First row of patient_notes dataset:"))
# patient_notes[1,]
# 
# print(paste("First row of test dataset:"))
# test[1,]
# 
# print(paste("View sample_submission dataset:"))
# sample_submission
```

```{r}
# merge_df <- merge(train, features, by=c("feature_num","case_num"), allow.cartesian=TRUE)
# final_df <- merge(merge_df, patient_notes, by=c("pn_num","case_num"), allow.cartesian=TRUE)
# 
# final_df$annotation_length <- nchar(final_df$annotation)
# 
# print(paste("The dimensions of the final_df dataset is", dim(final_df)[1],
#             "records by", dim(final_df)[2], "observations."))
# print(paste("There are", length(unique(final_df$case_num)),
#             "unique case numbers in the final_df dataset."))
# print(paste("There are", length(unique(final_df$feature_num)),
#             "unique feature numbers in the final_df dataset."))
# print(paste("There are", length(unique(final_df$pn_num)),
#             "unique patient note numbers in the final_df dataset."))
# print(paste("There are", length(unique(final_df$feature_text)),
#             "unique feature texts in the final_df dataset."))
# print(paste("There are", length(unique(final_df$annotation)),
#             "unique annotation texts in the final_df dataset."))
# print(paste("There are", length(unique(final_df$location)),
#             "unique locations in the final_df dataset."))
# print(paste("First row of the final_df dataset:"))
# final_df[1,]
# 
# id_00016_000_df <- final_df[final_df$id=="00016_000",]
# print(paste("The dimensions of id_00016_000_df are", dim(id_00016_000_df)[1],
#             "records by", dim(id_00016_000_df)[2], "observations."))
# print(paste("There are", length(unique(id_00016_000_df$case_num)),
#             "unique case numbers in id_00016_000_df."))
# print(paste("There are", length(unique(id_00016_000_df$feature_num)),
#             "unique feature numbers in id_00016_000_df."))
# print(paste("There are", length(unique(id_00016_000_df$pn_num)),
#             "unique patient note numbers in id_00016_000_df."))
# print(paste("There are", length(unique(id_00016_000_df$feature_text)),
#             "unique feature texts in id_00016_000_df."))
# print(paste("There are", length(unique(id_00016_000_df$annotation)),
#             "unique annotation texts in id_00016_000_df."))
# print(paste("There are", length(unique(id_00016_000_df$location)),
#             "unique locations in id_00016_000_df."))
# print(paste("id_00016_000_df:"))
```

```{r}
# require("NLP")
# ## Some text.
# s <- paste(c("Pierre Vinken, 61 years old, will join the board as a ",
#              "nonexecutive director Nov. 29.\n",
#              "Mr. Vinken is chairman of Elsevier N.V., ",
#              "the Dutch publishing group."),
#            collapse = "")
# s <- as.String(s)
# 
# ## Need sentence token annotations.
# sent_token_annotator <- Maxent_Sent_Token_Annotator()
# a1 <- annotate(s, sent_token_annotator)
# 
# word_token_annotator <- Maxent_Word_Token_Annotator()
# word_token_annotator
# a2 <- annotate(s, word_token_annotator, a1)
# a2
# ## Variant with word token probabilities as features.
# head(annotate(s, Maxent_Word_Token_Annotator(probs = TRUE), a1))
# 
# ## Can also perform sentence and word token annotations in a pipeline:
# a <- annotate(s, list(sent_token_annotator, word_token_annotator))
# head(a)
# 
# install.packages("opennlp", repos = "http://datacube.wu.ac.at/", type = "source")
```



```{r}
# # Offset mapping of sentences and words within id_00016_000_df$pn_history
# bio <- as.String(id_00016_000_df$pn_history)
# 
# # bio_annotations of id_00016_000_df$pn_history:
# word_ann <- Maxent_Word_Token_Annotator()
# sent_ann <- Maxent_Sent_Token_Annotator()
# bio_annotations <- annotate(bio, list(sent_ann, word_ann))
# 
# # Annotated sentences of id_00016_000_df$pn_history:
# bio_doc <- AnnotatedPlainTextDocument(bio, bio_annotations)
# 
# # Offset mapping to give the indices of each token
# df <- tibble(id=final_df$id, annotation=final_df$annotation)
# df$annotation <- gsub("[[:punct:]]", "", df$annotation)
# 
# indices <- df %>%
#   mutate(tokens = str_extract_all(annotation, "([^\\s]+)"),
#          locations = str_locate_all(annotation, "([^\\s]+)"),
#          locations = map(locations, as.data.frame)) %>%
#   select(-annotation) %>%
#   unnest(c(tokens, locations))
# print(paste("Indices of each token of final_df$annotation:"))
# head(indices)
# 
# # Offset mapping to give the indices of each annotation
# annotation_entire <- tibble(id=final_df$id, annotation=final_df$annotation)
# annotation_entire$annotation <- str_replace_all(annotation_entire$annotation, "[^[:alnum:]]", "")
# 
# indices_entire_annotation <- annotation_entire %>%
#   mutate(tokens = str_extract_all(annotation, "([^\\s]+)"),
#          locations = str_locate_all(annotation, "([^\\s]+)"),
#          locations = map(locations, as.data.frame)) %>%
#   select(-annotation) %>%
#   unnest(c(tokens, locations))
# print(paste("Indices of each annotation within final_df$annotation:"))
# head(indices_entire_annotation)
```

```{r}
# # Create Document Term Model (dtm)
# set.seed(475)
# project_data <- data.frame(final_df$pn_history)
# names(project_data) <- "text"
# 
# # Text Mining Functions
# dtmCorpus <- function(df) {
#   df_corpus <- Corpus(VectorSource(df$text))
#   df_corpus <- tm_map(df_corpus, function(x) iconv(x, to='ASCII'))
#   df_corpus <- tm_map(df_corpus, removeNumbers)
#   df_corpus <- tm_map(df_corpus, removePunctuation)
#   df_corpus <- tm_map(df_corpus, stripWhitespace)
#   df_corpus <- tm_map(df_corpus, tolower)
#   df_corpus <- tm_map(df_corpus, removeWords, stopwords('english'))
#   DocumentTermMatrix(df_corpus)
# }
# 
# dtm <- dtmCorpus(project_data)
```

```{r}
# # Set parameters for Gibbs sampling
# burnin <- 4000
# iter <- 2000
# thin <- 500
# seed <- list(2003,5,63,100001,765)
# nstart <- 5
# best <- TRUE
# 
# # Number of topics
# k <- 10
# 
# # Find the sum of words in each Document
# rowTotals <- apply(dtm, 1, sum)
# 
# # Remove all docs without words
# dtm.new <- dtm[rowTotals > 0,]
# 
# # Run LDA using Gibbs sampling
# ldaOut <- LDA(dtm.new, k, method="Gibbs", control=list(nstart=nstart, seed=seed, best=best, burnin=burnin, iter=iter, thin=thin))
# 
# # Docs to topics
# ldaOut.topics <- as.matrix(topics(ldaOut))
# 
# # Top 6 terms in each topic
# ldaOut.terms <- as.matrix(terms(ldaOut,10))
# 
# # Probabilities associated with each topic assignment
# topicProbabilities <- as.data.frame(ldaOut@gamma)
# 
# # Find probability of term being generated from topic
# ap_topics <- tidy(ldaOut, matrix = "beta")
# 
# # Table of beta spread per topic
# beta_spread <- ap_topics %>%
#   mutate(topic = paste0("topic", topic)) %>%
#   spread(topic, beta) %>%
#   filter(topic1 > .001 | topic2 > .001) %>%
#   mutate(log_ratio = log2(topic2 / topic1))
# 
# print(paste("Top terms for 5 topics:"))
# ap_top_terms <- ap_topics %>%
#   group_by(topic) %>%
#   top_n(5, beta) %>%
#   ungroup() %>%
#   arrange(topic, -beta)
# head(ap_top_terms)
# 
# print(paste("Table of topics per document:"))
# ap_documents <- tidy(ldaOut, matrix="gamma")
# head(ap_documents)
# 
# # STM Package processing to determine the optimal number of Topics.
# 
# # Process data using function textProcessor()
# processed <- textProcessor(project_data$text)
# out <- prepDocuments(processed$documents, processed$vocab,
#                      processed$meta, lower.thresh = 2)
```



# Final Notes:

In our conversation, we discussed various statistical analyses and their interpretations. It's essential to not only provide the output but also justify, interpret, and evaluate the results to draw meaningful conclusions. Simply providing the R output without context or interpretation doesn't provide any insight into the research question and the conclusions we can draw from it.For instance, when we were discussing the linear regression model, we not only looked at the coefficients and p-values but also interpreted them to understand the relationship between the independent and dependent variables. Similarly, in the hypothesis testing example, we not only looked at the p-value but also evaluated it based on the significance level and provided a conclusion based on the evidence. In summary, it's crucial to go beyond the output and provide a comprehensive evaluation and interpretation of the results to make informed conclusions and recommendations based on the research question. The RAF score will be calculated based on a subset of patient notes that have already been manually reviewed by trained physicians. These notes will serve as a gold standard for evaluating the performance of the automated system. The system will be tasked with identifying the same key clinical features as the manual review, and the RAF score will be used to assess how well it performs in doing so. A high RAF score would indicate that the automated system is able to accurately identify and present the relevant clinical features in a clear and fluent manner, with little to no errors. On the other hand, a low RAF score would suggest that there is room for improvement in the system's ability to accurately identify and present key clinical features.

Based on the evaluation of the RAF score, it was found that the LDA model outperformed the Naive Bayes model in accurately identifying the relevant features within the patient notes. This suggests that the LDA model was better at capturing the underlying patterns and relationships between the features, leading to more accurate and efficient identification. However, it is important to note that further evaluation and validation of the models should be conducted before drawing definitive conclusions about their effectiveness in improving the evaluation process of patient notes.



